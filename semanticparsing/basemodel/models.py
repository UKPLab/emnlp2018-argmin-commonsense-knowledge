"""
Neural models
"""

import keras
import numpy as np
from keras.engine import Input
from keras.engine import Model
from keras.layers.merge import concatenate, add, multiply
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Lambda

from semanticparsing.basemodel.attention_lstm import AttentionLSTM


def get_attention_lstm(word_index_to_embeddings_map, max_len, rich_context: bool=False, **kwargs):
    # converting embeddings to numpy 2d array: shape = (vocabulary_size, 300)
    embeddings = np.asarray([np.array(x, dtype=np.float32) for x in word_index_to_embeddings_map.values()])
    print('embeddings.shape', embeddings.shape)

    lstm_size = kwargs.get('lstm_size')
    dropout = kwargs.get('dropout')
    assert lstm_size
    assert dropout

    # define basic four input layers - for warrant0, warrant1, reason, claim
    sequence_layer_warrant0_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant0_input")
    sequence_layer_warrant1_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant1_input")
    sequence_layer_reason_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_reason_input")
    sequence_layer_claim_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_claim_input")
    sequence_layer_debate_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_debate_input")

    # now define embedded layers of the input
    embedded_layer_warrant0_input = Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_len, weights=[embeddings], mask_zero=True)(sequence_layer_warrant0_input)
    embedded_layer_warrant1_input = Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_len, weights=[embeddings], mask_zero=True)(sequence_layer_warrant1_input)
    embedded_layer_reason_input = Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_len, weights=[embeddings], mask_zero=True)(sequence_layer_reason_input)
    embedded_layer_claim_input = Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_len, weights=[embeddings], mask_zero=True)(sequence_layer_claim_input)
    embedded_layer_debate_input = Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_len, weights=[embeddings], mask_zero=True)(sequence_layer_debate_input)

    bidi_lstm_layer_reason = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Reason')(embedded_layer_reason_input)
    bidi_lstm_layer_claim = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Claim')(embedded_layer_claim_input)
    # add context to the attention layer
    bidi_lstm_layer_debate = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Context')(embedded_layer_debate_input)

    if rich_context:
        # merge reason and claim
        context_concat = concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_debate])
    else:
        context_concat = concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim])

    # max-pooling
    max_pool_lambda_layer = Lambda(lambda x: keras.backend.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))
    max_pool_lambda_layer.supports_masking = True
    attention_vector = max_pool_lambda_layer(context_concat)

    attention_warrant0 = AttentionLSTM(lstm_size)(embedded_layer_warrant0_input, constants=attention_vector)
    attention_warrant1 = AttentionLSTM(lstm_size)(embedded_layer_warrant1_input, constants=attention_vector)

    # concatenate them
    dropout_layer = Dropout(dropout)(add([attention_warrant0, attention_warrant1]))

    # and add one extra layer with ReLU
    dense1 = Dense(int(lstm_size / 2), activation='relu')(dropout_layer)
    output_layer = Dense(1, activation='sigmoid')(dense1)

    model = Model([sequence_layer_warrant0_input, sequence_layer_warrant1_input, sequence_layer_reason_input,
                   sequence_layer_claim_input, sequence_layer_debate_input], output=output_layer)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # from keras.utils.visualize_util import plot
    # plot(model, show_shapes=True, to_file='/tmp/model-att.png')

    # from keras.utils.visualize_util import plot
    # plot(model, show_shapes=True, to_file='/tmp/attlstm.png')

    return model


def get_attention_lstm_intra_warrant(word_index_to_embeddings_map, max_len, rich_context: bool=False, **kwargs):
    # converting embeddings to numpy 2d array: shape = (vocabulary_size, 300)
    embeddings = np.asarray([np.array(x, dtype=np.float32) for x in word_index_to_embeddings_map.values()])
    print('embeddings.shape', embeddings.shape)

    lstm_size = kwargs.get('lstm_size')
    dropout = kwargs.get('dropout')
    assert lstm_size
    assert dropout

    # define basic four input layers - for warrant0, warrant1, reason, claim
    sequence_layer_warrant0_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant0_input")
    sequence_layer_warrant1_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant1_input")
    sequence_layer_reason_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_reason_input")
    sequence_layer_claim_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_claim_input")
    sequence_layer_debate_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_debate_input")

    # now define embedded layers of the input
    word_emb_layer = Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_len, weights=[embeddings], mask_zero=True)
    embedded_layer_warrant0_input = word_emb_layer(sequence_layer_warrant0_input)
    embedded_layer_warrant1_input = word_emb_layer(sequence_layer_warrant1_input)
    embedded_layer_reason_input = word_emb_layer(sequence_layer_reason_input)
    embedded_layer_claim_input = word_emb_layer(sequence_layer_claim_input)
    embedded_layer_debate_input = word_emb_layer(sequence_layer_debate_input)

    bidi_lstm_layer_warrant0 = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-W0')(embedded_layer_warrant0_input)
    bidi_lstm_layer_warrant1 = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-W1')(embedded_layer_warrant1_input)
    bidi_lstm_layer_reason = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Reason')(embedded_layer_reason_input)
    bidi_lstm_layer_claim = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Claim')(embedded_layer_claim_input)
    # add context to the attention layer
    bidi_lstm_layer_debate = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Context')(embedded_layer_debate_input)

    # max-pooling
    max_pool_lambda_layer = Lambda(lambda x: keras.backend.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))
    max_pool_lambda_layer.supports_masking = True
    # two attention vectors

    if rich_context:
        attention_vector_for_w0 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant1, bidi_lstm_layer_debate]))
        attention_vector_for_w1 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant0, bidi_lstm_layer_debate]))
    else:
        attention_vector_for_w0 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant1]))
        attention_vector_for_w1 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant0]))

    attention_warrant0 = AttentionLSTM(lstm_size)(bidi_lstm_layer_warrant0, constants=attention_vector_for_w0)
    attention_warrant1 = AttentionLSTM(lstm_size)(bidi_lstm_layer_warrant1, constants=attention_vector_for_w1)


    # concatenate them
    dropout_layer = Dropout(dropout)(concatenate([add([attention_warrant0, attention_warrant1]), attention_warrant0, attention_warrant1]))

    # and add one extra layer with ReLU
    dense1 = Dense(int(lstm_size / 2), activation='relu')(dropout_layer)
    output_layer = Dense(1, activation='sigmoid')(dense1)

    model = Model([sequence_layer_warrant0_input, sequence_layer_warrant1_input, sequence_layer_reason_input,
                   sequence_layer_claim_input, sequence_layer_debate_input], output=output_layer)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # from keras.utils.visualize_util import plot
    # plot(model, show_shapes=True, to_file='/tmp/model-att.png')

    # from keras.utils.visualize_util import plot
    # plot(model, show_shapes=True, to_file='/tmp/attlstm.png')

    return model


def get_attention_lstm_intra_warrant_kb_tokens(word_index_to_embeddings_map,
                                                     max_len,
                                                     rich_context=False,
                                                     lstm_size=32,
                                                     warrant_lstm_size=32,
                                                     dropout=0.1,
                                                     kb_embeddings=None,
                                                     fn_embeddings=None):
    # converting embeddings to numpy 2d array: shape = (vocabulary_size, 300)
    embeddings = np.asarray([np.array(x, dtype=np.float32) for x in word_index_to_embeddings_map.values()])

    # define basic four input layers - for warrant0, warrant1, reason, claim
    sequence_layer_warrant0_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant0_input")
    sequence_layer_warrant1_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant1_input")
    sequence_layer_reason_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_reason_input")
    sequence_layer_claim_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_claim_input")
    sequence_layer_debate_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_debate_input")

    # now define embedded layers of the input
    word_emb_layer = Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_len, weights=[embeddings], mask_zero=True)
    embedded_layer_warrant0_input = word_emb_layer(sequence_layer_warrant0_input)
    embedded_layer_warrant1_input = word_emb_layer(sequence_layer_warrant1_input)
    embedded_layer_reason_input = word_emb_layer(sequence_layer_reason_input)
    embedded_layer_claim_input = word_emb_layer(sequence_layer_claim_input)
    embedded_layer_debate_input = word_emb_layer(sequence_layer_debate_input)

    if kb_embeddings is not None :
        sequence_layer_warrant0_input_kb = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant0_input_kb")
        sequence_layer_warrant1_input_kb = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant1_input_kb")
        sequence_layer_reason_input_kb = Input(shape=(max_len,), dtype='int32', name="sequence_layer_reason_input_kb")
        sequence_layer_claim_input_kb = Input(shape=(max_len,), dtype='int32', name="sequence_layer_claim_input_kb")

        kb_emb_layer = Embedding(kb_embeddings.shape[0], kb_embeddings.shape[1], input_length=max_len, weights=[kb_embeddings], mask_zero=True)
        embedded_layer_warrant0_input_kb = kb_emb_layer(sequence_layer_warrant0_input_kb)
        embedded_layer_warrant1_input_kb = kb_emb_layer(sequence_layer_warrant1_input_kb)
        embedded_layer_reason_input_kb = kb_emb_layer(sequence_layer_reason_input_kb)
        embedded_layer_claim_input_kb = kb_emb_layer(sequence_layer_claim_input_kb)

    if fn_embeddings is not None :
        sequence_layer_warrant0_input_fn = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant0_input_fn")
        sequence_layer_warrant1_input_fn = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant1_input_fn")
        sequence_layer_reason_input_fn = Input(shape=(max_len,), dtype='int32', name="sequence_layer_reason_input_fn")
        sequence_layer_claim_input_fn = Input(shape=(max_len,), dtype='int32', name="sequence_layer_claim_input_fn")

        fn_emb_layer = Embedding(fn_embeddings.shape[0], fn_embeddings.shape[1], input_length=max_len, weights=[fn_embeddings], mask_zero=True)
        embedded_layer_warrant0_input_fn = fn_emb_layer(sequence_layer_warrant0_input_fn)
        embedded_layer_warrant1_input_fn = fn_emb_layer(sequence_layer_warrant1_input_fn)
        embedded_layer_reason_input_fn = fn_emb_layer(sequence_layer_reason_input_fn)
        embedded_layer_claim_input_fn = fn_emb_layer(sequence_layer_claim_input_fn)

    if fn_embeddings is not None or kb_embeddings is not None:
        embedded_layer_warrant0_input = concatenate([embedded_layer_warrant0_input,
                                                 *((embedded_layer_warrant0_input_kb,) if kb_embeddings is not None  else ()),
                                                 *((embedded_layer_warrant0_input_fn,) if fn_embeddings is not None  else ()),
                                                 ])
        embedded_layer_warrant1_input = concatenate([embedded_layer_warrant1_input,
                                                     *((embedded_layer_warrant1_input_kb,) if kb_embeddings is not None  else ()),
                                                     *((embedded_layer_warrant1_input_fn,) if fn_embeddings is not None  else ())
                                                     ])
        embedded_layer_reason_input = concatenate([embedded_layer_reason_input,
                                                   *((embedded_layer_reason_input_kb,) if kb_embeddings is not None  else ()),
                                                   *((embedded_layer_reason_input_fn,) if fn_embeddings is not None  else ())
                                                   ])
        embedded_layer_claim_input = concatenate([embedded_layer_claim_input,
                                                  *((embedded_layer_claim_input_kb,) if kb_embeddings is not None  else ()),
                                                  *((embedded_layer_claim_input_fn,) if fn_embeddings is not None  else ())
                                                  ])

    bidi_lstm_layer_warrant0 = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-W0')(embedded_layer_warrant0_input)
    bidi_lstm_layer_warrant1 = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-W1')(embedded_layer_warrant1_input)
    bidi_lstm_layer_reason = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Reason')(embedded_layer_reason_input)
    bidi_lstm_layer_claim = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Claim')(embedded_layer_claim_input)
    # add context to the attention layer
    bidi_lstm_layer_debate = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Context')(embedded_layer_debate_input)

    # max-pooling
    max_pool_lambda_layer = Lambda(lambda x: keras.backend.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))
    max_pool_lambda_layer.supports_masking = True
    # two attention vectors

    if rich_context:
        attention_vector_for_w0 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant1, bidi_lstm_layer_debate]))
        attention_vector_for_w1 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant0, bidi_lstm_layer_debate]))
    else:
        attention_vector_for_w0 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant1]))
        attention_vector_for_w1 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant0]))

    attention_warrant0 = AttentionLSTM(lstm_size)(bidi_lstm_layer_warrant0, constants=attention_vector_for_w0)
    attention_warrant1 = AttentionLSTM(lstm_size)(bidi_lstm_layer_warrant1, constants=attention_vector_for_w1)

    # concatenate them
    dropout_layer = Dropout(dropout)(concatenate([add([attention_warrant0, attention_warrant1]), attention_warrant0, attention_warrant1]))

    # and add one extra layer with ReLU
    dense1 = Dense(int(warrant_lstm_size / 2), activation='relu')(dropout_layer)
    output_layer = Dense(1, activation='sigmoid')(dense1)

    model = Model([
        sequence_layer_warrant0_input,
        sequence_layer_warrant1_input,
        sequence_layer_reason_input,
        sequence_layer_claim_input,
        sequence_layer_debate_input,
        *(
            (sequence_layer_warrant0_input_kb,
            sequence_layer_warrant1_input_kb,
            sequence_layer_reason_input_kb,
            sequence_layer_claim_input_kb) if kb_embeddings is not None else ()),
        *(
            (sequence_layer_warrant0_input_fn,
             sequence_layer_warrant1_input_fn,
             sequence_layer_reason_input_fn,
             sequence_layer_claim_input_fn) if fn_embeddings is not None else ())

    ], output=output_layer)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model


def get_attention_lstm_intra_warrant_kb_pooled(word_index_to_embeddings_map,
                                                     max_len,
                                                     rich_context=False,
                                                     lstm_size=32,
                                                     warrant_lstm_size=32,
                                                     dropout=0.1,
                                                     kb_embeddings=None,
                                                     fn_embeddings=None):
    # converting embeddings to numpy 2d array: shape = (vocabulary_size, 300)
    embeddings = np.asarray([np.array(x, dtype=np.float32) for x in word_index_to_embeddings_map.values()])

    # max-pooling
    max_pool_lambda_layer = Lambda(lambda x: keras.backend.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))
    max_pool_lambda_layer.supports_masking = True
    # sum-pooling
    sum_pool_lambda_layer = Lambda(lambda x: keras.backend.sum(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))
    sum_pool_lambda_layer.supports_masking = True

    # define basic four input layers - for warrant0, warrant1, reason, claim
    sequence_layer_warrant0_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant0_input")
    sequence_layer_warrant1_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant1_input")
    sequence_layer_reason_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_reason_input")
    sequence_layer_claim_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_claim_input")
    sequence_layer_debate_input = Input(shape=(max_len,), dtype='int32', name="sequence_layer_debate_input")

    # now define embedded layers of the input
    word_emb_layer = Embedding(embeddings.shape[0], embeddings.shape[1], input_length=max_len, name='word_emb', weights=[embeddings], mask_zero=True)
    embedded_layer_warrant0_input = word_emb_layer(sequence_layer_warrant0_input)
    embedded_layer_warrant1_input = word_emb_layer(sequence_layer_warrant1_input)
    embedded_layer_reason_input = word_emb_layer(sequence_layer_reason_input)
    embedded_layer_claim_input = word_emb_layer(sequence_layer_claim_input)
    embedded_layer_debate_input = word_emb_layer(sequence_layer_debate_input)

    if kb_embeddings is not None:
        sequence_layer_warrant0_input_kb = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant0_input_kb")
        sequence_layer_warrant1_input_kb = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant1_input_kb")
        sequence_layer_reason_input_kb = Input(shape=(max_len,), dtype='int32', name="sequence_layer_reason_input_kb")
        sequence_layer_claim_input_kb = Input(shape=(max_len,), dtype='int32', name="sequence_layer_claim_input_kb")

        kb_emb_layer = Embedding(kb_embeddings.shape[0], kb_embeddings.shape[1], input_length=max_len, name='kb_emb_layer', weights=[kb_embeddings], mask_zero=True)
        embedded_layer_warrant0_input_kb = kb_emb_layer(sequence_layer_warrant0_input_kb)
        embedded_layer_warrant1_input_kb = kb_emb_layer(sequence_layer_warrant1_input_kb)
        embedded_layer_reason_input_kb = kb_emb_layer(sequence_layer_reason_input_kb)
        embedded_layer_claim_input_kb = kb_emb_layer(sequence_layer_claim_input_kb)

        kb_dense = Dense(lstm_size * 2, activation='relu')
        kb_vector_w0 = kb_dense(sum_pool_lambda_layer(concatenate([embedded_layer_reason_input_kb, embedded_layer_claim_input_kb, embedded_layer_warrant0_input_kb])))
        kb_vector_w1 = kb_dense(sum_pool_lambda_layer(concatenate([embedded_layer_reason_input_kb, embedded_layer_claim_input_kb, embedded_layer_warrant1_input_kb])))

    if fn_embeddings is not None :
        sequence_layer_warrant0_input_fn = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant0_input_fn")
        sequence_layer_warrant1_input_fn = Input(shape=(max_len,), dtype='int32', name="sequence_layer_warrant1_input_fn")
        sequence_layer_reason_input_fn = Input(shape=(max_len,), dtype='int32', name="sequence_layer_reason_input_fn")
        sequence_layer_claim_input_fn = Input(shape=(max_len,), dtype='int32', name="sequence_layer_claim_input_fn")

        fn_emb_layer = Embedding(fn_embeddings.shape[0], fn_embeddings.shape[1], input_length=max_len, name='fn_emb_layer', weights=[fn_embeddings], mask_zero=True)
        embedded_layer_warrant0_input_fn = fn_emb_layer(sequence_layer_warrant0_input_fn)
        embedded_layer_warrant1_input_fn = fn_emb_layer(sequence_layer_warrant1_input_fn)
        embedded_layer_reason_input_fn = fn_emb_layer(sequence_layer_reason_input_fn)
        embedded_layer_claim_input_fn = fn_emb_layer(sequence_layer_claim_input_fn)

        fn_dense = Dense(lstm_size * 2, activation='relu')
        fn_vector_w0 = fn_dense(sum_pool_lambda_layer(concatenate([embedded_layer_reason_input_fn, embedded_layer_claim_input_fn, embedded_layer_warrant0_input_fn])))
        fn_vector_w1 = fn_dense(sum_pool_lambda_layer(concatenate([embedded_layer_reason_input_fn, embedded_layer_claim_input_fn, embedded_layer_warrant1_input_fn])))

    bidi_lstm_layer_warrant0 = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-W0')(embedded_layer_warrant0_input)
    bidi_lstm_layer_warrant1 = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-W1')(embedded_layer_warrant1_input)
    bidi_lstm_layer_reason = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Reason')(embedded_layer_reason_input)
    bidi_lstm_layer_claim = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Claim')(embedded_layer_claim_input)
    # add context to the attention layer
    bidi_lstm_layer_debate = Bidirectional(LSTM(lstm_size, return_sequences=True), name='BiDiLSTM-Context')(embedded_layer_debate_input)

    # two attention vectors

    if rich_context:
        attention_vector_for_w0 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant1, bidi_lstm_layer_debate]))
        attention_vector_for_w1 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant0, bidi_lstm_layer_debate]))
    else:
        attention_vector_for_w0 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant1]))
        attention_vector_for_w1 = max_pool_lambda_layer(concatenate([bidi_lstm_layer_reason, bidi_lstm_layer_claim, bidi_lstm_layer_warrant0]))

    attention_warrant0 = AttentionLSTM(warrant_lstm_size)(bidi_lstm_layer_warrant0,
                                                          constants=concatenate([attention_vector_for_w0,
                                                                                 *((kb_vector_w0,) if kb_embeddings is not None  else ()),
                                                                                 *((fn_vector_w0,) if fn_embeddings is not None  else ()),
                                                                                 ]))
    attention_warrant1 = AttentionLSTM(warrant_lstm_size)(bidi_lstm_layer_warrant1,
                                                          constants=concatenate([attention_vector_for_w1,
                                                                                 *((kb_vector_w1,) if kb_embeddings is not None  else ()),
                                                                                 *((fn_vector_w1,) if fn_embeddings is not None  else ()),
                                                                                 ]))

    # concatenate them
    dropout_layer = Dropout(dropout)(concatenate([add([attention_warrant0, attention_warrant1]), attention_warrant0, attention_warrant1]))

    # and add one extra layer with ReLU
    dense1 = Dense(int(warrant_lstm_size / 2), activation='relu')(dropout_layer)
    output_layer = Dense(1, activation='sigmoid')(dense1)

    model = Model([
        sequence_layer_warrant0_input,
        sequence_layer_warrant1_input,
        sequence_layer_reason_input,
        sequence_layer_claim_input,
        sequence_layer_debate_input,
        *(
            (sequence_layer_warrant0_input_kb,
             sequence_layer_warrant1_input_kb,
             sequence_layer_reason_input_kb,
             sequence_layer_claim_input_kb) if kb_embeddings is not None else ()),
        *(
            (sequence_layer_warrant0_input_fn,
             sequence_layer_warrant1_input_fn,
             sequence_layer_reason_input_fn,
             sequence_layer_claim_input_fn) if fn_embeddings is not None else ())

    ], output=output_layer)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model
