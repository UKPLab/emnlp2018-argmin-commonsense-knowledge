import json
import os

import atexit
import numpy as np
from keras.preprocessing import sequence
from keras import callbacks

import hyperopt as hy

from semanticparsing.basemodel import data_loader, vocabulary_embeddings_extractor
from semanticparsing.basemodel.models import get_attention_lstm, get_attention_lstm_intra_warrant, \
    get_attention_lstm_intra_warrant_world_knowledge, get_attention_lstm_intra_warrant_kb_pooled
from semanticparsing.wikidata import kb_embeddings, annotation_loader as WD
from semanticparsing.framenet import fn_embeddings, annotation_loader as FN

max_len = 100  # padding length

optimization_space = {
    'dropout': hy.hp.uniform('dropout', .0, .75),
    'lstm_size': hy.hp.choice('lstm_size', [2**x for x in range(4, 9)]),
    'warrant_lstm_size': hy.hp.choice('warrant_lstm_size', [2**x for x in range(4, 9)]),
    'batch_size': hy.hp.choice('batch_size', [2**x for x in range(3, 6)]),
}

trials_counter = 0


def get_predicted_labels(predicted_probabilities):
    """
    Converts predicted probability/ies to label(s)
    @param predicted_probabilities: output of the classifier
    @return: labels as integers
    """
    assert isinstance(predicted_probabilities, np.ndarray)

    # if the output vector is a probability distribution, return the maximum value; otherwise
    # it's sigmoid, so 1 or 0
    if predicted_probabilities.shape[-1] > 1:
        predicted_labels_numpy = predicted_probabilities.argmax(axis=-1)
    else:
        predicted_labels_numpy = np.array([1 if p > 0.5 else 0 for p in predicted_probabilities])

    # check type
    assert isinstance(predicted_labels_numpy, np.ndarray)

    return predicted_labels_numpy


def __main__():
    # optional (and default values)
    verbose = 1

    print('Loading data...')

    current_dir = os.getcwd()
    embeddings_cache_file = current_dir + "/resources/embeddings_cache_file_word2vec.pkl.bz2"

    # load pre-extracted word-to-index maps and pre-filtered Glove embeddings
    word_to_indices_map, word_index_to_embeddings_map = \
        vocabulary_embeddings_extractor.load_cached_vocabulary_and_embeddings(embeddings_cache_file)

    entity_to_indices_map, entity_index_to_embeddings_map = kb_embeddings.load_kb_embeddings(current_dir + "/../entity-linking/data/WikidataEmb/dec_17_100/")
    frame_to_indices_map, frame_index_to_embeddings_map = fn_embeddings.load_fn_embeddings(current_dir + "/data/frameEmbeddings/dict_frame_to_emb_100dim_wsb_npArray.pkl")

    (train_instance_id_list, train_warrant0_list, train_warrant1_list, train_correct_label_w0_or_w1_list,
     train_reason_list, train_claim_list, train_debate_meta_data_list) = \
        data_loader.load_single_file(current_dir + '/data/arg-comprehension-full/train-full.txt', word_to_indices_map)

    train_warrant0_entities_list, train_warrant1_entities_list, train_reason_entities_list, train_claim_entities_list = \
        WD.load_single_file(current_dir + '/data/data-annotations/train_entitylinking.json', entity_to_indices_map)

    train_warrant0_frames_list, train_warrant1_frames_list, train_reason_frames_list, train_claim_frames_list = \
        FN.load_single_file(current_dir + '/data/data-annotations/train-full_predictions_with_lexicon_IH.pickle',
                            train_instance_id_list, frame_to_indices_map)

    (dev_instance_id_list, dev_warrant0_list, dev_warrant1_list, dev_correct_label_w0_or_w1_list,
     dev_reason_list, dev_claim_list, dev_debate_meta_data_list) = \
        data_loader.load_single_file(current_dir + '/data/arg-comprehension-full//dev-full.txt', word_to_indices_map)

    dev_warrant0_entities_list, dev_warrant1_entities_list, dev_reason_entities_list, dev_claim_entities_list = \
        WD.load_single_file(current_dir + '/data/data-annotations/dev_entitylinking.json', entity_to_indices_map)

    dev_warrant0_frames_list, dev_warrant1_frames_list, dev_reason_frames_list, dev_claim_frames_list = \
        FN.load_single_file(current_dir + '/data/data-annotations/dev-full_predictions_with_lexicon_IH.pickle',
                            dev_instance_id_list, frame_to_indices_map)

    # (test_instance_id_list, test_warrant0_list, test_warrant1_list, test_correct_label_w0_or_w1_list,
    #  test_reason_list, test_claim_list, test_debate_meta_data_list) = \
    #     data_loader.load_single_file(current_dir + '/data/arg-comprehension//test.tsv', word_to_indices_map)

    # pad all sequences
    (train_warrant0_list, train_warrant1_list, train_reason_list, train_claim_list, train_debate_meta_data_list,
     train_warrant0_entities_list, train_warrant1_entities_list, train_reason_entities_list, train_claim_entities_list,
     train_warrant0_frames_list, train_warrant1_frames_list, train_reason_frames_list, train_claim_frames_list) = [
        sequence.pad_sequences(x, maxlen=max_len) for x in
        (train_warrant0_list, train_warrant1_list, train_reason_list, train_claim_list, train_debate_meta_data_list,
         train_warrant0_entities_list, train_warrant1_entities_list, train_reason_entities_list, train_claim_entities_list,
         train_warrant0_frames_list, train_warrant1_frames_list, train_reason_frames_list, train_claim_frames_list)]

    # (test_warrant0_list, test_warrant1_list, test_reason_list, test_claim_list, test_debate_meta_data_list) = [
    #     sequence.pad_sequences(x, maxlen=max_len) for x in
    #     (test_warrant0_list, test_warrant1_list, test_reason_list, test_claim_list, test_debate_meta_data_list)]

    (dev_warrant0_list, dev_warrant1_list, dev_reason_list, dev_claim_list, dev_debate_meta_data_list,
     dev_warrant0_entities_list, dev_warrant1_entities_list, dev_reason_entities_list, dev_claim_entities_list,
     dev_warrant0_frames_list, dev_warrant1_frames_list, dev_reason_frames_list, dev_claim_frames_list) = [
        sequence.pad_sequences(x, maxlen=max_len) for x in
        (dev_warrant0_list, dev_warrant1_list, dev_reason_list, dev_claim_list, dev_debate_meta_data_list,
         dev_warrant0_entities_list, dev_warrant1_entities_list, dev_reason_entities_list, dev_claim_entities_list,
         dev_warrant0_frames_list, dev_warrant1_frames_list, dev_reason_frames_list, dev_claim_frames_list)]

    assert train_warrant0_list.shape == train_warrant1_list.shape == train_reason_list.shape == train_claim_list.shape \
           == train_debate_meta_data_list.shape == train_reason_entities_list.shape, train_claim_entities_list.shape

    trials = hy.Trials()
    atexit.register(lambda: wrap_up_optimization(trials))

    def train(sampled_parameters):
        global trials_counter
        lstm_size = sampled_parameters['lstm_size']  #64
        warrant_lstm_size = sampled_parameters['warrant_lstm_size']
        dropout = sampled_parameters['dropout']  #0.9  # empirically tested on dev set
        nb_epoch = 25
        batch_size = sampled_parameters['batch_size']  #32

        print(f'Trial: {trials_counter}, Trying: LSTM {lstm_size}, Warrant LSTM {warrant_lstm_size}, Dropout {dropout}, Batch {batch_size}')
        trials_counter += 1

        accs = []
        for i in range(1, 4):
            print("Run: ", i)

            np.random.seed(12345 + i)  # for reproducibility

            model = get_attention_lstm_intra_warrant_kb_pooled(word_index_to_embeddings_map, max_len, rich_context=True,
                                                                  dropout=dropout, lstm_size=lstm_size,
                                                                     warrant_lstm_size=warrant_lstm_size,
                                                                     kb_embeddings=None,
                                                                     fn_embeddings=frame_index_to_embeddings_map)
            model.fit(
                {'sequence_layer_warrant0_input': train_warrant0_list, 'sequence_layer_warrant1_input': train_warrant1_list,
                 'sequence_layer_reason_input': train_reason_list, 'sequence_layer_claim_input': train_claim_list,
                 'sequence_layer_debate_input': train_debate_meta_data_list,
                 'sequence_layer_warrant0_input_kb': train_warrant0_entities_list, 'sequence_layer_warrant1_input_kb': train_warrant1_entities_list,
                 'sequence_layer_reason_input_kb': train_reason_entities_list, 'sequence_layer_claim_input_kb': train_claim_entities_list,
                 'sequence_layer_warrant0_input_fn': train_warrant0_frames_list, 'sequence_layer_warrant1_input_fn': train_warrant1_frames_list,
                 'sequence_layer_reason_input_fn': train_reason_frames_list, 'sequence_layer_claim_input_fn': train_claim_frames_list},
                train_correct_label_w0_or_w1_list, epochs=nb_epoch, batch_size=batch_size, verbose=verbose,
                validation_split=0.1,
                callbacks=[callbacks.EarlyStopping(monitor="val_acc", patience=2, verbose=1),
                           callbacks.ModelCheckpoint("trainedmodels/model.kerasmodel",
                                                     monitor='val_acc', verbose=1, save_best_only=True)])

            model.load_weights("trainedmodels/model.kerasmodel")
            # model predictions
            predicted_probabilities_dev = model.predict(
                {'sequence_layer_warrant0_input': dev_warrant0_list, 'sequence_layer_warrant1_input': dev_warrant1_list,
                 'sequence_layer_reason_input': dev_reason_list, 'sequence_layer_claim_input': dev_claim_list,
                 'sequence_layer_debate_input': dev_debate_meta_data_list,
                 'sequence_layer_warrant0_input_kb': dev_warrant0_entities_list, 'sequence_layer_warrant1_input_kb': dev_warrant1_entities_list,
                 'sequence_layer_reason_input_kb': dev_reason_entities_list, 'sequence_layer_claim_input_kb': dev_claim_entities_list,
                 'sequence_layer_warrant0_input_fn': dev_warrant0_frames_list, 'sequence_layer_warrant1_input_fn': dev_warrant1_frames_list,
                 'sequence_layer_reason_input_fn': dev_reason_frames_list, 'sequence_layer_claim_input_fn': dev_claim_frames_list},
                batch_size=batch_size, verbose=1)

            predicted_labels_dev = get_predicted_labels(predicted_probabilities_dev)

            acc_dev = np.sum(np.asarray(dev_correct_label_w0_or_w1_list) == predicted_labels_dev) / len(dev_correct_label_w0_or_w1_list)
            print('Dev accuracy:', acc_dev)
            accs.append(acc_dev)
        acc = np.average(accs)
        print(f"Acc dev: {accs} -> {acc}")
        return {'acc': acc, 'accs': accs, 'loss': 1 - acc, 'status': hy.STATUS_OK, 'sampled.parameters': sampled_parameters}

    hy.fmin(train,
            optimization_space,
            algo=hy.rand.suggest,
            max_evals=50,
            trials=trials, verbose=1)


def wrap_up_optimization(trials):
    if len(trials.trials) > 0:
        print("Optimization finished, best trail: {}".format(trials.best_trial))
        print("Best parameters: {}".format(trials.best_trial['result']['sampled.parameters']))
        with open("trials.json", 'w') as out:
            json.dump([(t['misc']['vals'], t['result']) for t in trials.trials], out)


def print_error_analysis_dev(ids: set) -> None:
    """
    Prints instances given in the ids parameter; reads data from dev.tsv
    :param ids: ids
    :return: none
    """
    f = open('data/dev.tsv', 'r')
    lines = f.readlines()
    # remove first line with comments
    del lines[0]

    for line in lines:
        split_line = line.split('\t')
        # "#id warrant0 warrant1 correctLabelW0orW1 reason claim debateTitle debateInfo
        assert len(split_line) == 8

        instance_id = split_line[0]

        if instance_id in ids:
            print(line.strip())


if __name__ == "__main__":
    __main__()
