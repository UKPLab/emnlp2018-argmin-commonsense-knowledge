
import sys, json
import cv2
import torch

PATH_EL = "../entity-linking/"
sys.path.insert(0, PATH_EL)

import click
import tqdm
from pycorenlp import StanfordCoreNLP

from entitylinking import core as EL
from entitylinking.core.sentence import SentenceEncoder


corenlp = StanfordCoreNLP('http://semanticparsing:9000')
corenlp_properties = {
    'annotators': 'tokenize, pos, ner',
    'outputFormat': 'json'
}

EL.candidate_retrieval.entity_linking_p['max.match.diff'] = 0

EL.mention_extraction.np_parser = EL.mention_extraction.NgramNpParser(
                          exclude_pos={".", "ORDINAL", "TIME", "PERCENT", "NUMBER"},
                          exclude_if_first={"WDT", "WP", "WP$", "WRB", "VBZ", "VB", "VBP"},
                          exclude_prefix={"IN", "DT", "CC", "POS"},
                          exclude_suffix={"IN", "DT", "CC", "JJ", "RB", "JJR", "JJS", "RBR", "RBS"},
                          exclude_alone={"IN", "DT", "PDT", "POS", "PRP", "PRP$", "CC", "TO",
                                         "VBZ", "VBD", "VBP", "VB", "VBG", "VBN",
                                         "JJ", "RB", "JJR", "JJS", "RBR", "RBS",
                                         "MD", "WDT", "WP", "WP$", "WRB"
                                         })


@click.command()
@click.argument('path_to_file')
def apply(path_to_file):

    entitylinker = EL.MLLinker(path_to_model="../entity-linking/trainedmodels/VectorModel_137.torchweights",
                               confidence=0.01,
                               num_candidates=3,
                               max_mention_len=2)

    with open(path_to_file) as f:
        input_data = [l.strip().split("\t") for l in f.readlines()][1:]

    input_data = [[parts[i] for i in [1,2,4,5]] for parts in input_data]

    output_data = []
    for parts in tqdm.tqdm(input_data):
        output_per_story = []
        for s in parts:
            sent = entitylinker.link_entities_in_sentence_obj(EL.sentence.Sentence(input_text=s))
            sent.entities = [{k: e[k] for k in {'type', 'linkings', 'token_ids', 'poss', 'tokens', 'drop_score'}}
                             for e in sent.entities if len(e['linkings']) > 0]
            for e in sent.entities:
                e['linkings'] = [(l.get('kbID'), l.get('label')) for l in e['linkings']]
            output_per_story.append(sent)
        output_data.append(output_per_story)
    with open("data/dev_output_11_05.json", "w") as out:
        json.dump(output_data, out, sort_keys=True, indent=4, cls=SentenceEncoder)


if __name__ == '__main__':
    apply()
