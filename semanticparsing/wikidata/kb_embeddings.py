import numpy as np

all_zeroes = "ALL_ZERO"
unknown_el = "_UNKNOWN"
no_annotation = "_NO_ANNOTATION"


def load_kb_embeddings(path_to_folder):
    """
    Loads pre-trained KB embeddings from the specified path.

    @return (embeddings as an numpy array, entity2idx)
    """

    entity2idx = {}
    allowed_indices = set()
    with open("data/entity2id.filtered.txt", 'r') as f:
        for l in f.readlines():
            k, v, idx = tuple(l.strip().split("\t"))
            entity2idx[k] = int(idx) + 3
            allowed_indices.add(int(v))

    embeddings = []
    with open(path_to_folder + "/entity2vec.vec", 'r') as f:
        idx = 0
        for line in f.readlines():
            if idx in allowed_indices:
                split = line.strip().split('\t')
                embeddings.append([float(num) for num in split])
            idx += 1

    entity2idx[all_zeroes] = 0  # 0 is reserved for padding
    entity2idx[unknown_el] = 1  # 1 is reserved for OOV
    entity2idx[no_annotation] = 2  # 2 is reserved for no annotation tokens
    embedding_size = len(embeddings[0])
    vector_oov = 2 * 0.1 * np.random.rand(embedding_size) - 0.1
    vector_na = 2 * 0.1 * np.random.rand(embedding_size) - 0.1
    embeddings = np.asarray([[0.0]*embedding_size, vector_oov, vector_na] + embeddings, dtype='float32')

    print("KB embeddings loaded: {}".format(embeddings.shape))
    assert len(entity2idx) == len(embeddings)

    return entity2idx, embeddings
