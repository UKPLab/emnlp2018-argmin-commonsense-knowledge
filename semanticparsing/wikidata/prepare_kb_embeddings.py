"""
The knowledge base embedding matrix is extremely large. We preprocess teh entity list to load only relevant entities.
"""

import json


def prepare_embeddings():

    entity2idx = {}
    with open("../entity-linking/data/WikidataEmb/dec_17_50/" + "entity2id.txt", 'r') as f:
        f.readline()
        for l in f.readlines():
            k, v = tuple(l.strip().split("\t"))
            entity2idx[k] = int(v)


    with open("data/data-annotations/train_entitylinking.json") as f:
        data_annotations = json.load(f)

    with open("data/data-annotations/dev_entitylinking.json") as f:
        data_annotations += json.load(f)

    with open("data/data-annotations/test_entitylinking.json") as f:
        data_annotations += json.load(f)

    entity_ids = {e['linkings'][0][0] for l in data_annotations for s in l for e in s['entities'] if len(e['linkings']) > 0}

    entity2idx = [(k, v) for k, v in entity2idx.items() if k in entity_ids]
    print(len(entity2idx))

    entity2idx = [(k,v,i) for i, (k, v) in enumerate(entity2idx)]

    with open("data/entity2id.filtered.txt", 'w') as out:
        for t in entity2idx:
            out.write("\t".join([str(el) for el in t]) + "\n")


if __name__ == '__main__':
    prepare_embeddings()